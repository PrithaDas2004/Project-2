# Project-2
This project, titled Adaptive Auto-Sensitive Surveillance System Using LLM, is developed by a team of five members and integrates speech transcription, emotion detection, and environmental sound classification to enable intelligent audio analysis. We used OpenAI’s Whisper model for converting speech to text, a fine-tuned DistilRoBERTa model for detecting emotions such as anger, distress, or happiness, and a custom deep neural network trained on the ESC-50 dataset to recognize sounds like sirens, gunshots, and glass breaking. By combining these components, the system adopts a multi-modal approach to interpret audio more accurately and effectively. This makes it valuable for real-world applications such as emergency response, healthcare support, and customer service. For instance, in a fire scenario, the system can transcribe the caller's speech, detect panic in their voice, and identify critical background noises—providing actionable insights for faster and better decision-making.
